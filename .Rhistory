freq = sort(as.matrix(tdm2)[,2], decreasing = T)[1:20])
twitter_bigrams <- data.frame(
phrase = names(sort(as.matrix(tdm2)[,3], decreasing = T)[1:20]),
freq = sort(as.matrix(tdm2)[,3], decreasing = T)[1:20])
g2_blogs <- ggplot(blogs_bigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
coord_flip()
g2_news <- ggplot(news_bigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
coord_flip()
g2_twitter <- ggplot(twitter_bigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
coord_flip()
grid.arrange(g2_blogs, g2_news, g2_twitter, ncol=3)
g1_blogs <- ggplot(blogs_unigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
title("Blogs") +
coord_flip()
g1_blogs <- ggplot(blogs_unigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
title("Blogs") +
coord_flip()
g1_blogs <- ggplot(blogs_unigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
labs(title = "Blogs") +
coord_flip()
g1_blogs <- ggplot(blogs_unigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
labs(title = "Blogs") +
coord_flip()
g1_news <- ggplot(news_unigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
labs(title = "News") +
coord_flip()
g1_twitter <- ggplot(twitter_unigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
labs(title = "Twitter") +
coord_flip()
grid.arrange(g1_blogs, g1_news, g1_twitter, ncol=3)
g2_blogs <- ggplot(blogs_bigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
labs(title = "Blogs") +
coord_flip()
g2_news <- ggplot(news_bigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
labs(title = "News") +
coord_flip()
g2_twitter <- ggplot(twitter_bigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
labs(title = "Twitter") +
coord_flip()
grid.arrange(g2_blogs, g2_news, g2_twitter, ncol=3)
#Trigrams - most common 3-word phrases
blogs_trigrams <- data.frame(
phrase = names(sort(as.matrix(tdm3)[,1], decreasing = T)[1:20]),
freq = sort(as.matrix(tdm3)[,1], decreasing = T)[1:20])
news_trigrams <- data.frame(
phrase = names(sort(as.matrix(tdm3)[,2], decreasing = T)[1:20]),
freq = sort(as.matrix(tdm3)[,2], decreasing = T)[1:20])
twitter_trigrams <- data.frame(
phrase = names(sort(as.matrix(tdm3)[,3], decreasing = T)[1:20]),
freq = sort(as.matrix(tdm3)[,3], decreasing = T)[1:20])
g3_blogs <- ggplot(blogs_trigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
labs(title = "Blogs") +
coord_flip()
g3_news <- ggplot(news_trigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
labs(title = "News") +
coord_flip()
g3_twitter <- ggplot(twitter_trigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
labs(title = "Twitter") +
coord_flip()
grid.arrange(g3_blogs, g3_news, g3_twitter, ncol=3)
?available_data
available_data()
data("grady_augmented")
str(grady_augmented)
head(grady_augmented, 100)
"apple" in grady_augmented
"apple" %in% grady_augmented
library(dplyr)
names(as.matrix(tdm1))
as.matrix(tdm1)
View(as.matrix(tdm1))
rownames(as.matrix(tdm1))
tdm.df <- data.frame(
phrase = rownames(as.matrix(tdm1)),
as.matrix(tdm1)
)
View(tdm.df)
tdm.df <- tdm.df %>%
mutate(is.english = phrase %in% grady_augmented)
View(tdm.df)
?group_by
summarise()
?summarise
tdm.df <- tdm.df %>%
mutate(is.english = phrase %in% grady_augmented) %>%
group_by(is.english) %>%
summarise(sum_blogs = sum(blogs_sample),
sum_news = sum(blogs_news),
sum_twitter = sum(blogs_twitter))
tdm.df <- tdm.df %>%
mutate(is.english = phrase %in% grady_augmented) %>%
group_by(is.english) %>%
summarise(sum_blogs = sum(blogs_sample),
sum_news = sum(news_sample),
sum_twitter = sum(twitter_sample))
tdm.df
tdm.df <- data.frame(
phrase = rownames(as.matrix(tdm1)),
as.matrix(tdm1)
)
tdm.df <- tdm.df %>%
mutate(is.english = phrase %in% grady_augmented) %>%
group_by(is.english) %>%
summarise(sum_blogs = sum(blogs_sample),
sum_news = sum(news_sample),
sum_twitter = sum(twitter_sample)) %>%
transmute(prop_blogs = sum_blogs / sum(sum_blogs),
prop_news = sum_news / sum(sum_news),
prop_twitter = sum_twitter / sum(sum_twitter))
tdm.df
13950 / (195083 )
13950 / (195083 + 13950)
eng.words <- tdm.df %>%
mutate(is.english = phrase %in% grady_augmented) %>%
group_by(is.english) %>%
summarise(sum_blogs = sum(blogs_sample),
sum_news = sum(news_sample),
sum_twitter = sum(twitter_sample)) %>%
transmute(is.english = is.english,
prop_blogs = sum_blogs / sum(sum_blogs),
prop_news = sum_news / sum(sum_news),
prop_twitter = sum_twitter / sum(sum_twitter))
tdm.df <- data.frame(
phrase = rownames(as.matrix(tdm1)),
as.matrix(tdm1)
)
eng.words <- tdm.df %>%
mutate(is.english = phrase %in% grady_augmented) %>%
group_by(is.english) %>%
summarise(sum_blogs = sum(blogs_sample),
sum_news = sum(news_sample),
sum_twitter = sum(twitter_sample)) %>%
transmute(is.english = is.english,
prop_blogs = sum_blogs / sum(sum_blogs),
prop_news = sum_news / sum(sum_news),
prop_twitter = sum_twitter / sum(sum_twitter))
eng.words
eng.words <- tdm.df %>%
mutate(is.english = phrase %in% grady_augmented) %>%
group_by(is.english) %>%
summarise(sum_blogs = sum(blogs_sample),
sum_news = sum(news_sample),
sum_twitter = sum(twitter_sample)) %>%
transmute(is.english = is.english,
prop_blogs = sum_blogs / sum(sum_blogs),
prop_news = sum_news / sum(sum_news),
prop_twitter = sum_twitter / sum(sum_twitter)) %>%
gather()
?gather
?spread
library(tidyr)
eng.words <- tdm.df %>%
mutate(is.english = phrase %in% grady_augmented) %>%
group_by(is.english) %>%
summarise(sum_blogs = sum(blogs_sample),
sum_news = sum(news_sample),
sum_twitter = sum(twitter_sample)) %>%
transmute(is.english = is.english,
prop_blogs = sum_blogs / sum(sum_blogs),
prop_news = sum_news / sum(sum_news),
prop_twitter = sum_twitter / sum(sum_twitter)) %>%
gather()
eng.words
eng.words <- tdm.df %>%
mutate(is.english = phrase %in% grady_augmented) %>%
group_by(is.english) %>%
summarise(sum_blogs = sum(blogs_sample),
sum_news = sum(news_sample),
sum_twitter = sum(twitter_sample)) %>%
transmute(is.english = is.english,
prop_blogs = sum_blogs / sum(sum_blogs),
prop_news = sum_news / sum(sum_news),
prop_twitter = sum_twitter / sum(sum_twitter)) %>%
gather(is.english)
eng.words
?gather
tdm.df <- data.frame(
phrase = rownames(as.matrix(tdm1)),
as.matrix(tdm1)
)
eng.words <- tdm.df %>%
mutate(is.english = phrase %in% grady_augmented) %>%
group_by(is.english) %>%
summarise(sum_blogs = sum(blogs_sample),
sum_news = sum(news_sample),
sum_twitter = sum(twitter_sample)) %>%
transmute(is.english = is.english,
prop_blogs = sum_blogs / sum(sum_blogs),
prop_news = sum_news / sum(sum_news),
prop_twitter = sum_twitter / sum(sum_twitter)) %>%
gather(key = "is.english", value = "Percent", -is.english)
eng.words
eng.words <- tdm.df %>%
mutate(is.english = phrase %in% grady_augmented) %>%
group_by(is.english) %>%
summarise(sum_blogs = sum(blogs_sample),
sum_news = sum(news_sample),
sum_twitter = sum(twitter_sample)) %>%
transmute(is.english = is.english,
prop_blogs = sum_blogs / sum(sum_blogs),
prop_news = sum_news / sum(sum_news),
prop_twitter = sum_twitter / sum(sum_twitter)) %>%
gather(key = "Source", value = "Percent", -is.english)
eng.words
g_english <- ggplot(data = eng.words, aes(x = Source, y= Percent, fill = is.english)) +
geom_bar(stat = "identity")
g_english
eng.words <- tdm.df %>%
mutate(is.english = phrase %in% grady_augmented) %>%
group_by(is.english) %>%
summarise(sum_blogs = sum(blogs_sample),
sum_news = sum(news_sample),
sum_twitter = sum(twitter_sample)) %>%
transmute(is.english = is.english,
Blogs = sum_blogs / sum(sum_blogs),
News = sum_news / sum(sum_news),
Twitter = sum_twitter / sum(sum_twitter)) %>%
gather(key = "Source", value = "Percent", -is.english)
g_english <- ggplot(data = eng.words, aes(x = Source, y= Percent, fill = is.english)) +
geom_bar(stat = "identity")
g_english
g_english + geom_label()
g_english + geom_label(label = Percent)
g_english + geom_text(size = 3, position = position_stack(vjust = 0.5))
g_english <- ggplot(data = eng.words, aes(x = Source, y = Percent, fill = is.english, label = Percent)) +
geom_bar(stat = "identity") +
geom_text(size = 3, position = position_stack(vjust = 0.5))
g_english
g_english <- ggplot(data = eng.words, aes(x = Source, y = Percent, fill = is.english, label = Percent * 100)) +
geom_bar(stat = "identity") +
geom_text(size = 3, position = position_stack(vjust = 0.5))
g_english
g_english <- ggplot(data = eng.words, aes(x = Source, y = Percent, fill = is.english, label = round(Percent * 100), 0) +
g_english <- ggplot(data = eng.words,
aes(x = Source,
y = Percent,
fill = is.english,
label = round(Percent * 100, 0))) +
geom_bar(stat = "identity") +
geom_text(size = 3, position = position_stack(vjust = 0.5))
g_english
library(tm)
library(dplyr)
library(tidyr)
library(wordcloud)
library(ngram)
library(ggplot2)
library(lexicon)
library(RWeka)
library(gridExtra)
Sys.setenv(LANG = "en")
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(dplyr)
library(tidyr)
library(wordcloud)
library(ngram)
library(ggplot2)
library(lexicon)
library(RWeka)
library(gridExtra)
library(tm)
library(dplyr)
library(tidyr)
library(wordcloud)
library(ngram)
library(ggplot2)
library(lexicon)
library(RWeka)
library(gridExtra)
data.path <- choose.dir()
data.path <- choose.dir()
data.path
data.path <- "C:\\Users\\jakub.wiatrak\\Desktop\\final\\en_US"
collection <- VCorpus(DirSource(data.path, encoding = "UTF-8"))
summary(collection)
textDesc <- function(txt) {
#' returns basic statistics about a text - number of lines,
#' words, mean characters in a line and mean words in line
lines <- length(txt)
words <- wordcount(txt)
avgChar <- round(mean(nchar(txt)), 2)
avgWords <- round(words / lines, 2)
c(lines = lines, words = words, avgChar = avgChar, avgWords = avgWords)
}
rbind(c(title = collection[[1]]$meta$id, textDesc(collection[[1]]$content)),
c(title = collection[[2]]$meta$id, textDesc(collection[[2]]$content)),
c(title = collection[[3]]$meta$id, textDesc(collection[[3]]$content)))
set.seed(2137)
blogs_sample <- sample(collection[[1]]$content, 10000)
news_sample <- sample(collection[[2]]$content, 10000)
twitter_sample <- sample(collection[[3]]$content, 10000)
collection <- VCorpus(VectorSource(list(blogs_sample,
news_sample,
twitter_sample)))
collection[[1]]$meta$id <- "blogs_sample"
collection[[2]]$meta$id <- "news_sample"
collection[[3]]$meta$id <- "twitter_sample"
#transforming to all lowercase
collection <- tm_map(collection, content_transformer(tolower))
#removing stopwords
collection <- tm_map(collection, removeWords, stopwords())
#removing stopwords
collection <- tm_map(collection, removeWords, stopwords())
#removing profanities, using list of profanities from lexicon package
data("profanity_banned")
collection <- tm_map(collection, removeWords, profanity_banned)
#removing punctuation
collection <- tm_map(collection, removePunctuation, ucp = T)
#removing numbers, as they are less interesting in text mining
collection <- tm_map(collection, removeNumbers)
#removing dollarsigns, a very common character, but useless without numbers
subSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
collection <- tm_map(collection, subSpace, "\\$")
#stripping whitespaces
collection <- tm_map(collection, stripWhitespace)
?wordcloud
```{r}
#BLOGS
wordcloud(collection[[1]]$content, max.words = 100, random.order = FALSE,colors=brewer.pal(8, "Dark2"))
#NEWS
wordcloud(collection[[2]]$content, max.words = 100, random.order = FALSE,colors=brewer.pal(8, "Dark2"))
#TWITTER
wordcloud(collection[[3]]$content, max.words = 100, random.order = FALSE,colors=brewer.pal(8, "Dark2"))
#BLOGS
wordcloud(collection[[1]]$content, max.words = 30, random.order = FALSE,colors=brewer.pal(8, "Dark2"))
#NEWS
wordcloud(collection[[2]]$content, max.words = 30, random.order = FALSE,colors=brewer.pal(8, "Dark2"))
#TWITTER
wordcloud(collection[[3]]$content, max.words = 30, random.order = FALSE,colors=brewer.pal(8, "Dark2"))
#BLOGS
wordcloud(collection[[1]]$content, max.words = 30, random.order = FALSE,colors=brewer.pal(8, "Dark2"))
#NEWS
wordcloud(collection[[2]]$content, max.words = 30, random.order = FALSE,colors=brewer.pal(8, "Dark2"))
#TWITTER
wordcloud(collection[[3]]$content, max.words = 30, random.order = FALSE,colors=brewer.pal(8, "Dark2"))
#BLOGS
wordcloud(collection[[1]]$content, max.words = 50, random.order = FALSE,colors=brewer.pal(8, "Dark2"))
#NEWS
wordcloud(collection[[2]]$content, max.words = 50, random.order = FALSE,colors=brewer.pal(8, "Dark2"))
#TWITTER
wordcloud(collection[[3]]$content, max.words = 50, random.order = FALSE,colors=brewer.pal(8, "Dark2"))
#BLOGS
wordcloud(collection[[1]]$content, max.words = 40, random.order = FALSE,colors=brewer.pal(8, "Dark2"))
#NEWS
wordcloud(collection[[2]]$content, max.words = 40, random.order = FALSE,colors=brewer.pal(8, "Dark2"))
#TWITTER
wordcloud(collection[[3]]$content, max.words = 40, random.order = FALSE,colors=brewer.pal(8, "Dark2"))
UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
tdm1 <- TermDocumentMatrix(collection, control = list(tokenize = UnigramTokenizer))
tdm2 <- TermDocumentMatrix(collection, control = list(tokenize = BigramTokenizer))
tdm3 <- TermDocumentMatrix(collection, control = list(tokenize = TrigramTokenizer))
#Unigrams - most common words
blogs_unigrams <- data.frame(
phrase = names(sort(as.matrix(tdm1)[,1], decreasing = T)[1:20]),
freq = sort(as.matrix(tdm1)[,1], decreasing = T)[1:20])
news_unigrams <- data.frame(
phrase = names(sort(as.matrix(tdm1)[,2], decreasing = T)[1:20]),
freq = sort(as.matrix(tdm1)[,2], decreasing = T)[1:20])
twitter_unigrams <- data.frame(
phrase = names(sort(as.matrix(tdm1)[,3], decreasing = T)[1:20]),
freq = sort(as.matrix(tdm1)[,3], decreasing = T)[1:20])
g1_blogs <- ggplot(blogs_unigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
labs(title = "Blogs") +
coord_flip()
g1_news <- ggplot(news_unigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
labs(title = "News") +
coord_flip()
g1_twitter <- ggplot(twitter_unigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
labs(title = "Twitter") +
coord_flip()
grid.arrange(g1_blogs, g1_news, g1_twitter, ncol=3)
?grid.arrange
a <- wordcloud(collection[[1]]$content, max.words = 40, random.order = FALSE,colors=brewer.pal(8, "Dark2"))
#Bigrams - most common 2-word phrases
blogs_bigrams <- data.frame(
phrase = names(sort(as.matrix(tdm2)[,1], decreasing = T)[1:20]),
freq = sort(as.matrix(tdm2)[,1], decreasing = T)[1:20])
news_bigrams <- data.frame(
phrase = names(sort(as.matrix(tdm2)[,2], decreasing = T)[1:20]),
freq = sort(as.matrix(tdm2)[,2], decreasing = T)[1:20])
twitter_bigrams <- data.frame(
phrase = names(sort(as.matrix(tdm2)[,3], decreasing = T)[1:20]),
freq = sort(as.matrix(tdm2)[,3], decreasing = T)[1:20])
g2_blogs <- ggplot(blogs_bigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
labs(title = "Blogs") +
coord_flip()
g2_news <- ggplot(news_bigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
labs(title = "News") +
coord_flip()
g2_twitter <- ggplot(twitter_bigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
labs(title = "Twitter") +
coord_flip()
grid.arrange(g2_blogs, g2_news, g2_twitter, ncol=3)
#Trigrams - most common 3-word phrases
blogs_trigrams <- data.frame(
phrase = names(sort(as.matrix(tdm3)[,1], decreasing = T)[1:20]),
freq = sort(as.matrix(tdm3)[,1], decreasing = T)[1:20])
news_trigrams <- data.frame(
phrase = names(sort(as.matrix(tdm3)[,2], decreasing = T)[1:20]),
freq = sort(as.matrix(tdm3)[,2], decreasing = T)[1:20])
twitter_trigrams <- data.frame(
phrase = names(sort(as.matrix(tdm3)[,3], decreasing = T)[1:20]),
freq = sort(as.matrix(tdm3)[,3], decreasing = T)[1:20])
g3_blogs <- ggplot(blogs_trigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
labs(title = "Blogs") +
coord_flip()
g3_news <- ggplot(news_trigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
labs(title = "News") +
coord_flip()
g3_twitter <- ggplot(twitter_trigrams, aes(x = reorder(phrase, freq),y=freq)) +
geom_bar(stat = "identity") +
ylab("Freqency") +
xlab("Phrase") +
labs(title = "Twitter") +
coord_flip()
grid.arrange(g3_blogs, g3_news, g3_twitter, ncol=3)
tdm.df <- data.frame(
phrase = rownames(as.matrix(tdm1)),
as.matrix(tdm1)
)
eng.words <- tdm.df %>%
mutate(is.english = phrase %in% grady_augmented) %>%
group_by(is.english) %>%
summarise(sum_blogs = sum(blogs_sample),
sum_news = sum(news_sample),
sum_twitter = sum(twitter_sample)) %>%
transmute(is.english = is.english,
Blogs = sum_blogs / sum(sum_blogs),
News = sum_news / sum(sum_news),
Twitter = sum_twitter / sum(sum_twitter)) %>%
gather(key = "Source", value = "Percent", -is.english)
g_english <- ggplot(data = eng.words,
aes(x = Source,
y = Percent,
fill = is.english,
label = round(Percent * 100, 0))) +
geom_bar(stat = "identity") +
geom_text(size = 3, position = position_stack(vjust = 0.5))
g_english
g_english
