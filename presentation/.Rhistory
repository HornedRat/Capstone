S8,
A2_1:A2_9,
A5,
A6,
A7,
B1_1:B1_4,
C1_1:C1_10,
D10,
E1_1:E1_7) %>%
mutate(A2 = ((A2_1 + A2_2 + A2_3 + A2_4 + A2_5 + A2_6 + A2_7 + A2_8)>0 & A2_9 == 0),
B1 = ((B1_1 + B1_2 + B1_3) > 0),
E1 = ((E1_1 + E1_2 + E1_3 + E1_4 + E1_5 + E1_6)>0 & E1_7 == 0)) %>%
mutate(A2 = as.numeric(A2),
B1 = (as.numeric(B1),
E1 = as.numeric(E1)) %>%
select(ID,
HP_Country,
S6,
S7,
S1,
S2,
S5,
S8,
A2,
A5,
A6,
A7,
B1,
C1_3,
C1_1,
D10,
E1)
info <- taxpayers %>%
select(ID,
HP_Country,
S6,
S7,
S1,
S2,
S5,
S8,
A2_1:A2_9,
A5,
A6,
A7,
B1_1:B1_4,
C1_1:C1_10,
D10,
E1_1:E1_7) %>%
mutate(A2 = ((A2_1 + A2_2 + A2_3 + A2_4 + A2_5 + A2_6 + A2_7 + A2_8)>0 & A2_9 == 0),
B1 = ((B1_1 + B1_2 + B1_3) > 0),
E1 = ((E1_1 + E1_2 + E1_3 + E1_4 + E1_5 + E1_6)>0 & E1_7 == 0)) %>%
mutate(A2 = as.numeric(A2),
B1 = as.numeric(B1),
E1 = as.numeric(E1)) %>%
select(ID,
HP_Country,
S6,
S7,
S1,
S2,
S5,
S8,
A2,
A5,
A6,
A7,
B1,
C1_3,
C1_1,
D10,
E1)
View(info)
write.csv(info, file = "info.csv")
write.csv(info, file = "info.csv")
codebook <- labels[ID,
HP_Country,
S6,
S7,
S1,
S2,
S5,
S8,
A5,
A6,
A7,
D10]
View(taxpayers)
View(taxpayers)
View(taxpayers)
attributes(taxpayers)
names(attributes(taxpayers))
test <- read.spss(baza.dir)
levels(test[,'S1'])
test[,'S1']
levels(test['S1'])
test['S1']
class(test['S1'])
class(test[['S1']])
levels(test[['S1']])
rm(test)
baza.dir <- file.choose()
taxpayers <- read.spss(baza.dir, to.data.frame = T, use.value.labels = T)
labels <- attr(taxpayers, "variable.labels")
names <- attr(taxpayers, "names")
info <- taxpayers %>%
select(ID,
HP_Country,
S6,
S7,
S1,
S2,
S5,
S8,
A2_1:A2_9,
A5,
A6,
A7,
B1_1:B1_4,
C1_1:C1_10,
D10,
E1_1:E1_7)
info[,'A2_7']
as.character(info[,'A2_7'])
as.numeric(info[,'A2_7'])
info[,'A2_7']
info[,'A2_7']
info[,'A2_7'] == 2
info[,'A2_7'] == "Not Checked"
info <- taxpayers %>%
select(ID,
HP_Country,
S6,
S7,
S1,
S2,
S5,
S8,
A2_1:A2_9,
A5,
A6,
A7,
B1_1:B1_4,
C1_1:C1_10,
D10,
E1_1:E1_7) %>%
mutate(A2 = A2_9 == "Not Checked",
B1 = B2_4 == "Not Checked"),
E1 = E1_7 == "Not Checked") %>%
mutate(A2 = as.numeric(A2),
B1 = as.numeric(B1),
E1 = as.numeric(E1)) %>%
select(ID,
HP_Country,
S6,
S7,
S1,
S2,
S5,
S8,
A2,
A5,
A6,
A7,
B1,
C1_3,
C1_1,
D10,
E1)
write.csv(info, file = "info.csv")
info <- taxpayers %>%
select(ID,
HP_Country,
S6,
S7,
S1,
S2,
S5,
S8,
A2_1:A2_9,
A5,
A6,
A7,
B1_1:B1_4,
C1_1:C1_10,
D10,
E1_1:E1_7) %>%
mutate(A2 = A2_9 == "Not Checked",
B1 = B2_4 == "Not Checked",
E1 = E1_7 == "Not Checked") %>%
mutate(A2 = as.numeric(A2),
B1 = as.numeric(B1),
E1 = as.numeric(E1)) %>%
select(ID,
HP_Country,
S6,
S7,
S1,
S2,
S5,
S8,
A2,
A5,
A6,
A7,
B1,
C1_3,
C1_1,
D10,
E1)
info <- taxpayers %>%
select(ID,
HP_Country,
S6,
S7,
S1,
S2,
S5,
S8,
A2_1:A2_9,
A5,
A6,
A7,
B1_1:B1_4,
C1_1:C1_10,
D10,
E1_1:E1_7) %>%
mutate(A2 = A2_9 == "Not Checked",
B1 = B1_4 == "Not Checked",
E1 = E1_7 == "Not Checked") %>%
mutate(A2 = as.numeric(A2),
B1 = as.numeric(B1),
E1 = as.numeric(E1)) %>%
select(ID,
HP_Country,
S6,
S7,
S1,
S2,
S5,
S8,
A2,
A5,
A6,
A7,
B1,
C1_3,
C1_1,
D10,
E1)
write.csv(info, file = "info.csv")
test <- "Quick brown fox jumps over a lazy dog. I went to St. Nicholas' church. My mother finds movies, books etc. very boring."
install.packages("tokenizers")
library(tokenizers)
?tokenize_sentences
x <- tokenize_sentences(test)
x <- tokenize_sentences(test, simplify = T)
x
tokenize_sentences()
tokenize_sentences
showMethods(tokenize_sentences)
install.packages("openNLP")
library(openNLP)
?annotate
annotate
library(NLP)
y <- annotate(test, Maxent_Sent_Token_Annotator)
y <- annotate(test, Maxent_Sent_Token_Annotator())
View(y)
View(y)
test[y]
convert_text_to_sentences <- function(text, lang = "en") {
# Function to compute sentence annotations using the Apache OpenNLP Maxent sentence detector employing the default model for language 'en'.
sentence_token_annotator <- Maxent_Sent_Token_Annotator(language = lang)
# Convert text to class String from package NLP
text <- as.String(text)
# Sentence boundaries in text
sentence.boundaries <- annotate(text, sentence_token_annotator)
# Extract sentences
sentences <- text[sentence.boundaries]
# return sentences
return(sentences)
}
convert_text_to_sentences(test)
test
as.String(test)
as.String(test)[y]
shiny::runApp('C:/Users/jakub.wiatrak/Desktop/Capstone/app/TextPredApp')
runApp('C:/Users/jakub.wiatrak/Desktop/Capstone/app/TextPredApp')
runApp('C:/Users/jakub.wiatrak/Desktop/Capstone/app/TextPredApp')
runApp('C:/Users/jakub.wiatrak/Desktop/Capstone/app/TextPredApp')
runApp('C:/Users/jakub.wiatrak/Desktop/Capstone/app/TextPredApp')
runApp('C:/Users/jakub.wiatrak/Desktop/Capstone/app/TextPredApp')
runApp('C:/Users/jakub.wiatrak/Desktop/Capstone/app/TextPredApp')
runApp('C:/Users/jakub.wiatrak/Desktop/Capstone/app/TextPredApp')
runApp('C:/Users/jakub.wiatrak/Desktop/Capstone/app/TextPredApp')
runApp('C:/Users/jakub.wiatrak/Desktop/Capstone/app/TextPredApp')
runApp('C:/Users/jakub.wiatrak/Desktop/Capstone/app/TextPredApp')
runApp('C:/Users/jakub.wiatrak/Desktop/Capstone/app/TextPredApp')
runApp('C:/Users/jakub.wiatrak/Desktop/Capstone/app/TextPredApp')
runApp('C:/Users/jakub.wiatrak/Desktop/Capstone/app/TextPredApp')
runApp('C:/Users/jakub.wiatrak/Desktop/Capstone/app/TextPredApp')
runApp('C:/Users/jakub.wiatrak/Desktop/Capstone/app/TextPredApp')
runApp('C:/Users/jakub.wiatrak/Desktop/Capstone/app/TextPredApp')
runApp('C:/Users/jakub.wiatrak/Desktop/Capstone/app/TextPredApp')
library(dplyr)
load(file = "SBOdata")
cleanText <- function(str) {
#converts toLower, leaves only letters, strips whitespaces
out <- tolower(str)
out <- gsub("[^a-z ']","", out)
out <- gsub("[ ]+", " ", out)
out <- gsub(" +$", "", out)
return(out)
}
getWords <- function(str, n = 3) {
# gets the last n words from a text input
v <- strsplit(str, split = " ")[[1]]
if (length(v) < n) {
out <- v
} else {
out <- v[(length(v) - n + 1):length(v)]
}
out
}
matchNgrams <- function(pre, data = SBOdata) {
#pre - one or more words (as a char vector) (from getWords())
words <- pre[pre != "" & !is.na(pre)]
n <- length(words) + 1
matched <- data[[n]]
if(n>1) {
for(i in 1:length(words)) {
word <- words[i]
matched <- filter(matched, matched[[i]] == word)
}
}
out <- matched[,(ncol(matched)-1):ncol(matched)]
names(out)[1] <- "word"
out
}
getNmatches <- function(phrase, data = SBOdata) {
#for length 0 and 1 it will return the same thing
words <- phrase[phrase != "" & !is.na(phrase)]
if(length(words)==0) {
matched <- data[[1]]
} else {
matched <- data[[length(words)]]
for(i in 1:length(words)) {
word <- words[i]
matched <- filter(matched, matched[[i]] == word)
}
}
sum(matched$n, na.rm = T)
}
SBOpredict <- function(pre, lambda = 0.5, data = SBOdata) {
#predicts the next word using a SBO - Stupid Back Off
#returns 3 most probable words
#pre - preceding words (from getWords function)
#lambda - a back-off parameter
len <- length(pre)
#initializing an empty tibble
predictions <- tibble(word = character(), score = numeric())
# backs off, starting with using all words in pre
# through using -1 word, finishing on unigrams
for(i in 1:(len + 1)) {
ngram <- pre[i:(len + 1)]
occur <- getNmatches(ngram)
if(occur > 0) {
new_preds <- matchNgrams(ngram) %>%
mutate(score = (n / occur) * (lambda^(i-1))) %>%
select(-n) %>%
top_n(3, score)
predictions <- bind_rows(predictions, anti_join(new_preds, predictions, by="word"))
}
}
top_n(predictions, 3, wt=score)
}
predict_from_text <- function(text) {
input <- cleanText(text)
input <- getWords(input)
SBOpredict(input)
}
Rprof()
predict_from_text("hello world, i am the great")
library(dplyr)
load(file = "SBOdata")
cleanText <- function(str) {
#converts toLower, leaves only letters, strips whitespaces
out <- tolower(str)
out <- gsub("[^a-z ']","", out)
out <- gsub("[ ]+", " ", out)
out <- gsub(" +$", "", out)
return(out)
}
getWords <- function(str, n = 3) {
# gets the last n words from a text input
v <- strsplit(str, split = " ")[[1]]
if (length(v) < n) {
out <- v
} else {
out <- v[(length(v) - n + 1):length(v)]
}
out
}
matchNgrams <- function(pre, data = SBOdata) {
#pre - one or more words (as a char vector) (from getWords())
words <- pre[pre != "" & !is.na(pre)]
n <- length(words) + 1
matched <- data[[n]]
if(n>1) {
for(i in 1:length(words)) {
word <- words[i]
matched <- filter(matched, matched[[i]] == word)
}
}
out <- matched[,(ncol(matched)-1):ncol(matched)]
names(out)[1] <- "word"
out
}
getNmatches <- function(phrase, data = SBOdata) {
#for length 0 and 1 it will return the same thing
words <- phrase[phrase != "" & !is.na(phrase)]
if(length(words)==0) {
matched <- data[[1]]
} else {
matched <- data[[length(words)]]
for(i in 1:length(words)) {
word <- words[i]
matched <- filter(matched, matched[[i]] == word)
}
}
sum(matched$n, na.rm = T)
}
SBOpredict <- function(pre, lambda = 0.5, data = SBOdata) {
#predicts the next word using a SBO - Stupid Back Off
#returns 3 most probable words
#pre - preceding words (from getWords function)
#lambda - a back-off parameter
len <- length(pre)
#initializing an empty tibble
predictions <- tibble(word = character(), score = numeric())
# backs off, starting with using all words in pre
# through using -1 word, finishing on unigrams
for(i in 1:(len + 1)) {
ngram <- pre[i:(len + 1)]
occur <- getNmatches(ngram)
if(occur > 0) {
new_preds <- matchNgrams(ngram) %>%
mutate(score = (n / occur) * (lambda^(i-1))) %>%
select(-n) %>%
top_n(3, score)
predictions <- bind_rows(predictions, anti_join(new_preds, predictions, by="word"))
}
}
top_n(predictions, 3, wt=score)
}
predict_from_text <- function(text) {
input <- cleanText(text)
input <- getWords(input)
SBOpredict(input)
}
load(file = "SBOdata")
setwd("C:/Users/jakub.wiatrak/Desktop/Capstone/app/TextPredApp")
load(file = "SBOdata")
Rprof()
predict_from_text("hello, I am the great")
summaryRprof()
times <- numeric()
sys.time()
Sys.time()
predict_from_text <- function(text) {
before <- Sys.time()
input <- cleanText(text)
input <- getWords(input)
SBOpredict(input)
times <<- c(times, Sys.time() - before)
}
predict_from_text("hello, I am the great")
times <- numeric()
predict_from_text <- function(text) {
before <- Sys.time()
input <- cleanText(text)
input <- getWords(input)
times <<- c(times, Sys.time() - before)
SBOpredict(input)
}
predict_from_text("hello, I am the great")
times <- numeric()
predict_from_text <- function(text) {
before <- Sys.time()
input <- cleanText(text)
input <- getWords(input)
return(SBOpredict(input))
times <<- c(times, Sys.time() - before)
}
predict_from_text("We are ready to move forward with the study ")
predict_from_text("We are ready to move forward with the study ")
predict_from_text <- function(text) {
before <- Sys.time()
input <- cleanText(text)
input <- getWords(input)
out <- SBOpredict(input)
times <<- c(times, Sys.time() - before)
out
}
predict_from_text("We are ready to move forward with the study ")
predict_from_text("Going forward, for these sort of “housekeeping” items, do we need to ")
predict_from_text("Overall story you have so far, and the SCQR (or point out where we can find this on MS Teams), so we can better familiarize")
predict_from_text("-	Develop first draft of ")
predict_from_text("I am working on an innovation survey for the North America M&C Marketing Lead – it will ")
mean(times)
setwd("C:/Users/jakub.wiatrak/Desktop/Capstone/presentation")
