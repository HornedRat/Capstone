out
}
matchNgrams <- function(pre, data = SBOdata) {
#pre - one or more words (as a char vector) (from getWords())
words <- pre[pre != "" & !is.na(pre)]
n <- length(words) + 1
matched <- data[[n]]
if(n>1) {
for(i in 1:length(words)) {
word <- words[i]
matched <- filter(matched, matched[[i]] == word)
}
}
out <- matched[,(ncol(matched)-1):ncol(matched)]
names(out)[1] <- "word"
out
}
getNmatches <- function(phrase, data = SBOdata) {
#for length 0 and 1 it will return the same thing
words <- phrase[phrase != "" & !is.na(phrase)]
if(length(words)==0) {
matched <- data[[1]]
} else {
matched <- data[[length(words)]]
for(i in 1:length(words)) {
word <- words[i]
matched <- filter(matched, matched[[i]] == word)
}
}
sum(matched$n, na.rm = T)
}
SBOpredict <- function(pre, lambda = 0.1, data = SBOdata) {
#predicts the next word using a SBO - Stupid Back Off
#returns 3 most probable words
#pre - preceding words (from getWords function)
#lambda - a back-off parameter
len <- length(pre)
#initializing an empty tibble
predictions <- tibble(word = character(), score = numeric())
# backs off, starting with using all words in pre
# through using -1 word, finishing on unigrams
for(i in 1:(len + 1)) {
ngram <- pre[i:(len + 1)]
occur <- getNmatches(ngram)
if(occur > 0) {
new_preds <- matchNgrams(ngram) %>%
mutate(score = (n / occur) * (lambda^(i-1))) %>%
select(-n) %>%
top_n(10, score)
predictions <- bind_rows(predictions, anti_join(new_preds, predictions, by="word"))
}
}
top_n(predictions, 10, wt=score)
}
predict_from_text <- function(text) {
input <- cleanText(text)
input <- getWords(input)
SBOpredict(input)
}
predict_from_text("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd")
predict_from_text("Guy at my table's wife got up to go to the bathroom and I asked about dessert and he started telling me about his")
predict_from_text("I'd give anything to see arctic monkeys this")
predict_from_text("Talking to your mom has the same effect as a hug and helps reduce your")
predict_from_text("When you were in Holland you were like 1 inch away from me but you hadn't time to take a")
predict_from_text("I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the")
predict_from_text("I can't deal with unsymetrical things. I can't even hold an uneven number of bags of groceries in each")
predict_from_text("Every inch of you is perfect from the bottom to the")
predict_from_text("Iâ€™m thankful my childhood was filled with imagination and bruises from playing")
predict_from_text("Question 10
I like how the same people are in almost all of Adam Sandler's")
#Ngrams - solution for to big ram usage
library(dplyr)
library(tidytext)
lines_at_a_time <- 20000
#TDMs - to nie dzia?a
source <- file("preprocessed files/unigrams_all.txt", "r")
linesread = lines_at_a_time
linesSoFar = 0
while(linesread == lines_at_a_time) {
t <- readLines(source, lines_at_a_time)
linesread <- length(t)
linesSoFar <- linesSoFar + linesread
print(paste("Lines read:", linesSoFar))
}
tdm1 <- tibble(phrase = character(), n = numeric())
nrow(tdm1)
test <- tibble(a = 1:100)
readLines(test, 10)
?connection
test <- tibble(a = 1:100, b = c("a", "b"))
test <- tibble(a = 1:100, b = 1000:1100)
test <- tibble(a = 1:100, b = 1001:1100)
out <- file("preprocessed files/test.csv")
writeLines(out, test)
close(out)
out <- file("preprocessed files/test.txt")
writeLines(out, test)
write.csv(test, "test.csv")
write.csv(test, "test.csv", row.names = F)
con <- file("test.csv", "r")
readLines(con)
read.csv(con)
read.table(con)
read.csv(readLines(con))
read.csv(test.csv)
read.csv("test.csv")
con <- file("test.csv", "r")
read.csv(file = con)
read.csv(file = con, 10)
read.csv(file = con)
con <- file("test.csv", "r")
read.csv(file = con, 10)
read.csv(file = con, 10)
close(con)
lines_at_a_time <- 100000
linesread = lines_at_a_time
linesSoFar = 0
source <- file("preprocessed files/unigrams_all.txt", "r")
tdm1 <- tibble(phrase = character(), n = numeric())
while(linesread == lines_at_a_time) {
t <- readLines(source, lines_at_a_time)
linesread <- length(t)
t <- tibble(phrase = t)
tdm <- t %>%
count(phrase)
tdm1 <- bind_rows(tdm1, tdm)
linesSoFar <- linesSoFar + linesread
print(paste("Lines read:", linesSoFar))
}
tdm1 <- tdm1 %>%
count(tdm1)
tdm1 <- tdm1 %>%
count(phrase)
length(unique(tdm1$phrase))
tdm1
test(c(rep("a", 50), rep("b", 25)))
test <- c(rep("a", 50), rep("b", 25))
test <- tibble(phrase = c(rep("a", 50), rep("b", 25)))
test_counted <- count(test, phrase)
View(test_counted)
rbind(test_counted, c("a", 3))
test_counted <- rbind(test_counted, c("a", 3))
View(test_counted)
count(test_counted, phrase)
count(test_counted, phrase, wt = n)
count(test_counted, phrase, wt = 'n')
?count
count(test_counted, wt = 'n')
count(test_counted, phrase, wt = test_counted$n)
View(test_counted)
test_counted$n <- as.numeric(test_counted$n)
count(test_counted, phrase)
count(test_counted, phrase, wt = n)
close(source)
source <- file("preprocessed files/unigrams_all.txt", "r")
tdm1 <- tibble(phrase = character(), n = numeric())
lines_at_a_time <- 100000
linesread = lines_at_a_time
linesSoFar = 0
while(linesread == lines_at_a_time) {
t <- readLines(source, lines_at_a_time)
linesread <- length(t)
t <- tibble(phrase = t)
tdm <- t %>%
count(phrase)
tdm1 <- bind_rows(tdm1, tdm)
linesSoFar <- linesSoFar + linesread
print(paste("Lines read:", linesSoFar))
}
tdm1 <- tdm1 %>%
count(phrase, wt = n)
tdm1
tdm1 <- arrange(tdm1, desc(n))
tdm1
save(tdm1, file = "unigrams_all_tdm")
save(tdm1, file = "preprocessed files/unigrams_all_tdm")
source <- file("preprocessed files/bigrams_all.txt", "r")
tdm2 <- tibble(phrase = character(), n = numeric())
lines_at_a_time <- 100000
linesread = lines_at_a_time
linesSoFar = 0
while(linesread == lines_at_a_time) {
t <- readLines(source, lines_at_a_time)
linesread <- length(t)
t <- tibble(phrase = t)
tdm <- t %>%
count(phrase)
tdm2 <- bind_rows(tdm2, tdm)
linesSoFar <- linesSoFar + linesread
print(paste("Lines read:", linesSoFar))
}
close(source)
tdm2 <- tdm2 %>%
count(phrase, wt = n) %>%
arrange(desc(n))
save(tdm2, file = "preprocessed files/bigrams_all_tdm")
#Ngrams - solution for to big ram usage
library(dplyr)
library(tidytext)
source <- file("preprocessed files/trigrams_all.txt", "r")
tdm3 <- tibble(phrase = character(), n = numeric())
lines_at_a_time <- 500000
linesread = lines_at_a_time
linesSoFar = 0
while(linesread == lines_at_a_time) {
t <- readLines(source, lines_at_a_time)
linesread <- length(t)
t <- tibble(phrase = t)
tdm <- t %>%
count(phrase)
tdm3 <- bind_rows(tdm3, tdm)
linesSoFar <- linesSoFar + linesread
print(paste("Lines read:", linesSoFar))
}
file.remove("test.csv")
a <- tibble(a = 1:100)
library(dplyr)
a <- tibble(a = 1:100)
write.csv(a, file = "temp/a.csv")
dir.create("temp")
write.csv(a, file = "temp/a.csv")
dir.create("temp")
write.csv(a, file = "temp/a.csv")
unlink("temp")
unlink("temp", recursive = T)
source <- file("preprocessed files/trigrams_all.txt", "r")
lines_at_a_time <- 500000
linesread = lines_at_a_time
linesSoFar = 0
chunksCreated = 0
dir.create("temp")
#chunking
while(linesread == lines_at_a_time) {
t <- readLines(source, lines_at_a_time)
linesread <- length(t)
t <- tibble(phrase = t)
tdm <- t %>%
count(phrase)
write.csv(tdm,
file = paste0("temp/chunk",chunksCreated, ".csv"),
row.names = F)
linesSoFar <- linesSoFar + linesread
chunksCreated <- chunksCreated + 1
print(paste("Lines read:", linesSoFar))
print(paste("Chunks created:", chunksCreated))
}
close(source)
load("preprocessed files/sentences_all")
str(sentences)
sentences$sentence[grep("i'd live and i'd", sentences$sentence)]
sentences$sentence[grep("live and i'd", sentences$sentence)]
sentences$sentence[grep("live and", sentences$sentence)]
sentences$sentence[grep("live and i", sentences$sentence)]
sentences$sentence[grep("live and i'd", sentences$sentence)]
sentences$sentence[grep("i'd", sentences$sentence)]
sentences$sentence[grep("and i'd", sentences$sentence)]
sentences$sentence[grep("live and i'd", sentences$sentence)]
library(dplyr)
load(file = "SBOdata")
cleanText <- function(str) {
#converts toLower, leaves only letters, strips whitespaces
out <- tolower(str)
out <- gsub("[^a-z ']","", out)
out <- gsub("[ ]+", " ", out)
out <- gsub(" +$", "", out)
return(out)
}
getWords <- function(str, n = 4) {
# gets the last n words from a text input
v <- strsplit(str, split = " ")[[1]]
out <- v[(length(v) - n + 1):length(v)]
out
}
matchNgrams <- function(pre, data = SBOdata) {
#pre - one or more words (as a char vector) (from getWords())
words <- pre[pre != "" & !is.na(pre)]
n <- length(words) + 1
matched <- data[[n]]
if(n>1) {
for(i in 1:length(words)) {
word <- words[i]
matched <- filter(matched, matched[[i]] == word)
}
}
out <- matched[,(ncol(matched)-1):ncol(matched)]
names(out)[1] <- "word"
out
}
getNmatches <- function(phrase, data = SBOdata) {
#for length 0 and 1 it will return the same thing
words <- phrase[phrase != "" & !is.na(phrase)]
if(length(words)==0) {
matched <- data[[1]]
} else {
matched <- data[[length(words)]]
for(i in 1:length(words)) {
word <- words[i]
matched <- filter(matched, matched[[i]] == word)
}
}
sum(matched$n, na.rm = T)
}
SBOpredict <- function(pre, lambda = 0.1, data = SBOdata) {
#predicts the next word using a SBO - Stupid Back Off
#returns 3 most probable words
#pre - preceding words (from getWords function)
#lambda - a back-off parameter
len <- length(pre)
#initializing an empty tibble
predictions <- tibble(word = character(), score = numeric())
# backs off, starting with using all words in pre
# through using -1 word, finishing on unigrams
for(i in 1:(len + 1)) {
ngram <- pre[i:(len + 1)]
occur <- getNmatches(ngram)
if(occur > 0) {
new_preds <- matchNgrams(ngram) %>%
mutate(score = (n / occur) * (lambda^(i-1))) %>%
select(-n) %>%
top_n(10, score)
predictions <- bind_rows(predictions, anti_join(new_preds, predictions, by="word"))
}
}
top_n(predictions, 10, wt=score)
}
predict_from_text <- function(text) {
input <- cleanText(text)
input <- getWords(input)
SBOpredict(input)
}
predict_from_text("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd")
predict_from_text("Guy at my table's wife got up to go to the bathroom and I asked about dessert and he started telling me about his")
sentences$sentence[grep("and i'd die", sentences$sentence)]
sentences$sentence[grep("and i'd sleep", sentences$sentence)]
sentences$sentence[grep("and i'd give", sentences$sentence)]
sentences$sentence[grep("and i'd eat", sentences$sentence)]
sentences$sentence[grep("about his financial", sentences$sentence)]
sentences$sentence[grep("about his horticultural", sentences$sentence)]
sentences$sentence[grep("about his spiritual", sentences$sentence)]
sentences$sentence[grep("about his marital", sentences$sentence)]
sentences$sentence[grep("arctic monkeys this", sentences$sentence)]
sentences$sentence[grep("monkeys this", sentences$sentence)]
length(sentences$sentence[grep("this weekend", sentences$sentence)])
length(sentences$sentence[grep("this month", sentences$sentence)])
length(sentences$sentence[grep("this morning", sentences$sentence)])
length(sentences$sentence[grep("this decade", sentences$sentence)])
sentences$sentence[grep("helps reduce your", sentences$sentence)]
sentences$sentence[grep("reduce your", sentences$sentence)]
length(sentences$sentence[grep("reduce your stress", sentences$sentence)])
length(sentences$sentence[grep("reduce your hunger", sentences$sentence)])
length(sentences$sentence[grep("reduce your happiness", sentences$sentence)])
length(sentences$sentence[grep("reduce your sleepiness", sentences$sentence)])
predict_from_text("I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the")
sentences$sentence[grep("settle the", sentences$sentence)]
sentences$sentence[grep("settle the ", sentences$sentence)]
length(sentences$sentence[grep("settle the matter", sentences$sentence)])
length(sentences$sentence[grep("settle the account", sentences$sentence)])
length(sentences$sentence[grep("settle the incident", sentences$sentence)])
length(sentences$sentence[grep("settle the case", sentences$sentence)])
length(sentences$sentence[grep("in each ", sentences$sentence)])
length(sentences$sentence[grep("in each hand", sentences$sentence)])
length(sentences$sentence[grep("in each arm", sentences$sentence)])
length(sentences$sentence[grep("playing daily", sentences$sentence)])
length(sentences$sentence[grep("playing outside", sentences$sentence)])
length(sentences$sentence[grep("playing inside", sentences$sentence)])
length(sentences$sentence[grep("playing weekly", sentences$sentence)])
sentences$sentence[grep("Adam Sandler's", sentences$sentence)]
sentences$sentence[grep("Sandler's", sentences$sentence)]
library(shiny)
ui <- fluidPage(
textAreaInput("caption", "Caption", "Data Summary", width = "1000px"),
verbatimTextOutput("value")
)
server <- function(input, output) {
output$value <- renderText({ input$caption })
}
shinyApp(ui, server)
?textAreaInput
runApp('app/TextPredApp')
?fluidPage
runApp('app/TextPredApp')
runApp('app/TextPredApp')
?column
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
ui <- fluidPage(
fluidRow(
column(12,
"Fluid 12",
fluidRow(
column(6,
"Fluid 6",
fluidRow(
column(6,
"Fluid 6"),
column(6,
"Fluid 6")
)
),
column(width = 6,
"Fluid 6")
)
)
)
)
if (interactive()) {
# Apps can be run without a server.r and ui.r file
runApp(list(
ui = ui,
server = function(input, output) {
}
))
runApp(app)
}
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
?textAreaInput
runApp('app/TextPredApp')
?textAreaInput
?column
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
?validateCssUnit
validateCssUnit("10%")
?textInput
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
runApp('app/TextPredApp')
ui <- bootstrapPage(
uiOutput('Button')
)
server <- function(input, output) {
# store the counter outside your input/button
vars = reactiveValues(counter = 0)
output$Button <- renderUI({
actionButton("click", label = label())
})
# increase the counter
observe({
if(!is.null(input$click)){
input$click
isolate({
vars$counter <- vars$counter + 1
})
}
})
label <- reactive({
if(!is.null(input$click)){
if(vars$counter >= 2) label <- "new label"
else label <- "old label"
}
})
}
# run the app
shinyApp(ui = ui, server = server)
ui =(pageWithSidebar(
headerPanel("Test Shiny App"),
sidebarPanel(
textInput("sample_text", "test", value = "0"),
#display dynamic UI
uiOutput("my_button")),
mainPanel()
))
server = function(input, output, session){
#make dynamic button
output$my_button <- renderUI({
actionButton("action", label = input$sample_text)
})
}
runApp(list(ui = ui, server = server))
?actionButton
