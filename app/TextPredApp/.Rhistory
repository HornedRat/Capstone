}
top_n(predictions, 3, wt=score)
}
predict_from_text <- function(text) {
input <- cleanText(text)
input <- getWords(input)
SBOpredict(input)
}
predict_from_text("I don't know what to do next")
cleanText <- function(str) {
#converts toLower, leaves only letters, strips whitespaces
out <- tolower(str)
out <- gsub("[^a-z ']","", out)
out <- gsub("[ ]+", " ", out)
out <- gsub(" +$", "", out)
return(out)
}
getWords <- function(str, n = 4) {
# gets the last n words from a text input
v <- strsplit(str, split = " ")[[1]]
out <- v[(length(v) - n + 1):length(v)]
out
}
matchNgrams <- function(pre, data = SBOdata) {
#pre - one or more words (as a char vector) (from getWords())
words <- pre[pre != "" & !is.na(pre)]
n <- length(words) + 1
matched <- data[[n]]
if(n>1) {
for(i in 1:length(words)) {
word <- words[i]
matched <- filter(matched, matched[[i]] == word)
}
}
out <- matched[,(ncol(matched)-1):ncol(matched)]
names(out)[1] <- "word"
out
}
getNmatches <- function(phrase, data = SBOdata) {
#for length 0 and 1 it will return the same thing
words <- phrase[phrase != "" & !is.na(phrase)]
if(length(words)==0) {
matched <- data[[1]]
} else {
matched <- data[[length(words)]]
for(i in 1:length(words)) {
word <- words[i]
matched <- filter(matched, matched[[i]] == word)
}
}
sum(matched$n, na.rm = T)
}
SBOpredict <- function(pre, lambda = 0.4, data = SBOdata) {
#predicts the next word using a SBO - Stupid Back Off
#returns 3 most probable words
#pre - preceding words (from getWords function)
#lambda - a back-off parameter
len <- length(pre)
#initializing an empty tibble
predictions <- tibble(word = character(), score = numeric())
# backs off, starting with using all words in pre
# through using -1 word, finishing on unigrams
for(i in 1:(len + 1)) {
ngram <- pre[i:(len + 1)]
occur <- getNmatches(ngram)
if(occur > 0) {
new_preds <- matchNgrams(ngram) %>%
mutate(score = (n / occur) * (lambda^(i-1))) %>%
select(-n) %>%
top_n(3, score)
predictions <- bind_rows(predictions, anti_join(new_preds, predictions, by="word"))
}
}
top_n(predictions, 3, wt=score)
}
predict_from_text <- function(text) {
input <- cleanText(text)
input <- getWords(input)
SBOpredict(input)
}
predict_from_text("I don't know what to do next")
debug(SBOpredict)
predict_from_text("I don't know what to do next")
undebug(SBOpredict)
SBOpredict <- function(pre, lambda = 0.9, data = SBOdata) {
#predicts the next word using a SBO - Stupid Back Off
#returns 3 most probable words
#pre - preceding words (from getWords function)
#lambda - a back-off parameter
len <- length(pre)
#initializing an empty tibble
predictions <- tibble(word = character(), score = numeric())
# backs off, starting with using all words in pre
# through using -1 word, finishing on unigrams
for(i in 1:(len + 1)) {
ngram <- pre[i:(len + 1)]
occur <- getNmatches(ngram)
if(occur > 0) {
new_preds <- matchNgrams(ngram) %>%
mutate(score = (n / occur) * (lambda^(i-1))) %>%
select(-n) %>%
top_n(3, score)
predictions <- bind_rows(predictions, anti_join(new_preds, predictions, by="word"))
}
}
top_n(predictions, 3, wt=score)
}
predict_from_text("I don't know what to do next")
cleanText <- function(str) {
#converts toLower, leaves only letters, strips whitespaces
out <- tolower(str)
out <- gsub("[^a-z ']","", out)
out <- gsub("[ ]+", " ", out)
out <- gsub(" +$", "", out)
return(out)
}
getWords <- function(str, n = 3) {
# gets the last n words from a text input
v <- strsplit(str, split = " ")[[1]]
out <- v[(length(v) - n + 1):length(v)]
out
}
matchNgrams <- function(pre, data = SBOdata) {
#pre - one or more words (as a char vector) (from getWords())
words <- pre[pre != "" & !is.na(pre)]
n <- length(words) + 1
matched <- data[[n]]
if(n>1) {
for(i in 1:length(words)) {
word <- words[i]
matched <- filter(matched, matched[[i]] == word)
}
}
out <- matched[,(ncol(matched)-1):ncol(matched)]
names(out)[1] <- "word"
out
}
getNmatches <- function(phrase, data = SBOdata) {
#for length 0 and 1 it will return the same thing
words <- phrase[phrase != "" & !is.na(phrase)]
if(length(words)==0) {
matched <- data[[1]]
} else {
matched <- data[[length(words)]]
for(i in 1:length(words)) {
word <- words[i]
matched <- filter(matched, matched[[i]] == word)
}
}
sum(matched$n, na.rm = T)
}
SBOpredict <- function(pre, lambda = 0.4, data = SBOdata) {
#predicts the next word using a SBO - Stupid Back Off
#returns 3 most probable words
#pre - preceding words (from getWords function)
#lambda - a back-off parameter
len <- length(pre)
#initializing an empty tibble
predictions <- tibble(word = character(), score = numeric())
# backs off, starting with using all words in pre
# through using -1 word, finishing on unigrams
for(i in 1:(len + 1)) {
ngram <- pre[i:(len + 1)]
occur <- getNmatches(ngram)
if(occur > 0) {
new_preds <- matchNgrams(ngram) %>%
mutate(score = (n / occur) * (lambda^(i-1))) %>%
select(-n) %>%
top_n(3, score)
predictions <- bind_rows(predictions, anti_join(new_preds, predictions, by="word"))
}
}
top_n(predictions, 3, wt=score)
}
predict_from_text <- function(text) {
input <- cleanText(text)
input <- getWords(input)
SBOpredict(input)
}
predict_from_text("I don't know what to do next")
cleanText <- function(str) {
#converts toLower, leaves only letters, strips whitespaces
out <- tolower(str)
out <- gsub("[^a-z ']","", out)
out <- gsub("[ ]+", " ", out)
out <- gsub(" +$", "", out)
return(out)
}
getWords <- function(str, n = 2) {
# gets the last n words from a text input
v <- strsplit(str, split = " ")[[1]]
out <- v[(length(v) - n + 1):length(v)]
out
}
matchNgrams <- function(pre, data = SBOdata) {
#pre - one or more words (as a char vector) (from getWords())
words <- pre[pre != "" & !is.na(pre)]
n <- length(words) + 1
matched <- data[[n]]
if(n>1) {
for(i in 1:length(words)) {
word <- words[i]
matched <- filter(matched, matched[[i]] == word)
}
}
out <- matched[,(ncol(matched)-1):ncol(matched)]
names(out)[1] <- "word"
out
}
getNmatches <- function(phrase, data = SBOdata) {
#for length 0 and 1 it will return the same thing
words <- phrase[phrase != "" & !is.na(phrase)]
if(length(words)==0) {
matched <- data[[1]]
} else {
matched <- data[[length(words)]]
for(i in 1:length(words)) {
word <- words[i]
matched <- filter(matched, matched[[i]] == word)
}
}
sum(matched$n, na.rm = T)
}
SBOpredict <- function(pre, lambda = 0.4, data = SBOdata) {
#predicts the next word using a SBO - Stupid Back Off
#returns 3 most probable words
#pre - preceding words (from getWords function)
#lambda - a back-off parameter
len <- length(pre)
#initializing an empty tibble
predictions <- tibble(word = character(), score = numeric())
# backs off, starting with using all words in pre
# through using -1 word, finishing on unigrams
for(i in 1:(len + 1)) {
ngram <- pre[i:(len + 1)]
occur <- getNmatches(ngram)
if(occur > 0) {
new_preds <- matchNgrams(ngram) %>%
mutate(score = (n / occur) * (lambda^(i-1))) %>%
select(-n) %>%
top_n(3, score)
predictions <- bind_rows(predictions, anti_join(new_preds, predictions, by="word"))
}
}
top_n(predictions, 3, wt=score)
}
predict_from_text <- function(text) {
input <- cleanText(text)
input <- getWords(input)
SBOpredict(input)
}
predict_from_text("I don't know what to do next")
cleanText <- function(str) {
#converts toLower, leaves only letters, strips whitespaces
out <- tolower(str)
out <- gsub("[^a-z ']","", out)
out <- gsub("[ ]+", " ", out)
out <- gsub(" +$", "", out)
return(out)
}
getWords <- function(str, n = 2) {
# gets the last n words from a text input
v <- strsplit(str, split = " ")[[1]]
out <- v[(length(v) - n + 1):length(v)]
out
}
matchNgrams <- function(pre, data = SBOdata) {
#pre - one or more words (as a char vector) (from getWords())
words <- pre[pre != "" & !is.na(pre)]
n <- length(words) + 1
matched <- data[[n]]
if(n>1) {
for(i in 1:length(words)) {
word <- words[i]
matched <- filter(matched, matched[[i]] == word)
}
}
out <- matched[,(ncol(matched)-1):ncol(matched)]
names(out)[1] <- "word"
out
}
getNmatches <- function(phrase, data = SBOdata) {
#for length 0 and 1 it will return the same thing
words <- phrase[phrase != "" & !is.na(phrase)]
if(length(words)==0) {
matched <- data[[1]]
} else {
matched <- data[[length(words)]]
for(i in 1:length(words)) {
word <- words[i]
matched <- filter(matched, matched[[i]] == word)
}
}
sum(matched$n, na.rm = T)
}
SBOpredict <- function(pre, lambda = 0.5, data = SBOdata) {
#predicts the next word using a SBO - Stupid Back Off
#returns 3 most probable words
#pre - preceding words (from getWords function)
#lambda - a back-off parameter
len <- length(pre)
#initializing an empty tibble
predictions <- tibble(word = character(), score = numeric())
# backs off, starting with using all words in pre
# through using -1 word, finishing on unigrams
for(i in 1:(len + 1)) {
ngram <- pre[i:(len + 1)]
occur <- getNmatches(ngram)
if(occur > 0) {
new_preds <- matchNgrams(ngram) %>%
mutate(score = (n / occur) * (lambda^(i-1))) %>%
select(-n) %>%
top_n(3, score)
predictions <- bind_rows(predictions, anti_join(new_preds, predictions, by="word"))
}
}
top_n(predictions, 3, wt=score)
}
predict_from_text <- function(text) {
input <- cleanText(text)
input <- getWords(input)
SBOpredict(input)
}
predict_from_text("I don't know what to do next")
SBOdata <- SBOdata[1:3]
predict_from_text("I don't know what to do next")
save(SBOdata, file = "SBOdata")
predictions <- predict_from_text("i want to break free")
runApp()
runApp()
?actionButton
runApp()
prediction1 <- "..."
runApp()
runApp()
runApp()
runApp()
?reactive
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
prediction1 <- reactiveVal()
prediction1 <- 'aaaa'
prediction1 <- reactiveVal()
prediction1() <- 'aaa'
?reactiveVal
prediction1(aaaa)
prediction1('aaaa')
prediction1()
rm(prediction1
)
runApp()
runApp()
?helpText
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
library(dplyr)
library(tidytext)
library(tidyr)
read_txt_tibble <- function(path = file.choose(), encoding = "UTF-8") {
#interactively reads txt files into tibbles
vector <- readLines(path, encoding = encoding)
tibble <- tibble(line = 1:length(vector), text = vector)
tibble
}
blogs <- read_txt_tibble("C:\\Users\\jakub.wiatrak\\Desktop\\final\\en_US\\en_US.blogs.txt")
news <- read_txt_tibble("C:\\Users\\jakub.wiatrak\\Desktop\\final\\en_US\\en_US.news.txt")
twitter <- read_txt_tibble("C:\\Users\\jakub.wiatrak\\Desktop\\final\\en_US\\en_US.twitter.txt")
#merging data
data <- bind_rows(mutate(blogs, source = "blogs"),
mutate(news, source = "news"),
mutate(twitter, source = "twitter"))
rm(blogs, news, twitter)
set.seed(2137)
data <- sample_frac(data, size = 0.5)
data <- data %>%
mutate(text =  gsub("[^\x20-\x7E]", "", text)) %>%
mutate(text = gsub("[ ]+", " ", text))
#tokenizing into sentences
#we do that in order to avoid trigrams spanning more than 1 sentence
sentences <- data %>%
unnest_tokens(sentence, text, token = "sentences")
rm(data)
#cleaning punctuation and numbers
sentences <- sentences %>%
mutate(sentence = gsub("[^a-z ']","", sentence)) %>%
mutate(sentence = gsub("[ ]+", " ", sentence)) %>%
mutate(sentence = gsub(" +$", "", sentence))
gc()
SBOdata <- list()
#tokenizing into n-grams (up to 3) and converting it into TDMs
unigrams <- sentences %>%
unnest_tokens(word_1, sentence, token = "words") %>%
filter(!is.na(word_1)) %>%
group_by(word_1) %>%
summarise(n = n()) %>%
filter(n > 2) %>%
arrange(desc(n))
SBOdata[[1]] <- unigrams
rm(unigrams)
gc()
bigrams <- sentences %>%
unnest_tokens(phrase, sentence, token = "ngrams", n = 2) %>%
filter(!is.na(phrase)) %>%
group_by(phrase) %>%
summarise(n = n()) %>%
filter(n > 3) %>%
arrange(desc(n)) %>%
separate(phrase, into = c("word_1", "word_2"), sep = " ")
SBOdata[[2]] <- bigrams
rm(bigrams)
gc()
bigrams <- sentences %>%
unnest_tokens(phrase, sentence, token = "ngrams", n = 2) %>%
filter(!is.na(phrase)) %>%
group_by(phrase) %>%
summarise(n = n()) %>%
filter(n > 2) %>%
arrange(desc(n)) %>%
separate(phrase, into = c("word_1", "word_2"), sep = " ")
SBOdata[[2]] <- bigrams
rm(bigrams)
gc()
trigrams <- sentences %>%
unnest_tokens(phrase, sentence, token = "ngrams", n = 3) %>%
filter(!is.na(phrase)) %>%
group_by(phrase) %>%
summarise(n = n()) %>%
arrange(desc(n)) %>%
filter(n > 2) %>%
separate(phrase, into = c("word_1", "word_2", "word_3"), sep = " ")
SBOdata[[3]] <- trigrams
rm(trigrams)
gc()
save(SBOdata, file = "SBOdata")
quadrigrams <- sentences %>%
unnest_tokens(phrase, sentence, token = "ngrams", n = 4) %>%
filter(!is.na(phrase)) %>%
group_by(phrase) %>%
summarise(n = n()) %>%
filter(n > 2) %>%
arrange(desc(n)) %>%
separate(phrase, into = c("word_1", "word_2", "word_3", "word_4"), sep = " ")
SBOdata[[4]] <- quadrigrams
rm(quadrigrams)
save(SBOdata, file = "SBOdata")
shiny::runApp()
install.packages("shinythemes")
View(SBOdata)
load("SBOdata_smaller")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
traceback()
shiny::runApp()
shiny::runApp()
predict_from_text("fdsafg adsa dsss")
runApp()
debug(getWords)
runApp()
undebug(getWords)
predict_from_text("fdsafg")
strsplit("fdsafg", split = " ")[[1]]
getWords <- function(str, n = 3) {
# gets the last n words from a text input
v <- strsplit(str, split = " ")[[1]]
if (length(v) < n) {
out <- v
} else {
out <- v[(length(v) - n + 1):length(v)]
}
out
}
predict_from_text("fdsafg")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
